{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Before going to any Machine Learning Algorithm, we need to do Data Preprocessing steps\n",
    "    - Import dataset from the CSV file.  \n",
    "    - Split Feature variable (X) & Depended variable (Y).  \n",
    "    - Replace missing data.  \n",
    "    - Replace string or categorical data.  \n",
    "    - Feature scaling.  \n",
    "    - Split training and test dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset from the CSV file. \n",
    "- We can download realtime pratice dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- We are using CSV as our dataset file & with the help of the pandas module, we can easily read CSV and have it in a variable called a Dataframe. \n",
    "- Dataframe is a cool type provided by pandas where we can do complex operations on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "dataset = pd.read_csv(r\"../dataset/Data.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Feature variable (X) & Depended variable (Y).  \n",
    "\n",
    "- Feature variable, in-depended variable, input variable are represented as X in the code, which helps to predict Y.\n",
    "- Depended variable, output variable are represented as Y in the code, which is the expected prediction from the X.  \n",
    "- Most commonly the last column will be the dependent variable Y rest of them will be feature variable X.\n",
    "- So, we are splitting the given dataset into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "Y\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values # [row, column]\n",
    "Y = dataset.iloc[:, -1].values\n",
    "print(\"X\", X, \"Y\", Y, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace missing data\n",
    "- As you can see there are some of the cells are empty and they are filled with ```nan``` \n",
    "- We need to eliminate or replace those empty cells with the ```strategy```\n",
    "- Yes, there are many ```strategy``` like [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n",
    "- But, we are going to use a mean strategy to replace all the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imputer.fit(X[:, 1:2+1])\n",
    "X[:, 1:2+1] = imputer.transform(X[:, 1:2+1])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace string data\n",
    "- We can't perform any algorithm with the string data so we need to convert those string data into some other algorithm understandable format\n",
    "- Before that, we can understand that you dataset has two type of string data columns which are [\"Country\", \"Purchased\"]\n",
    "- Where, ```Country``` is the categorical string data where only 3 category countries are repeating again and again.\n",
    "- Whereas, ```Purchased``` is the label string data where it is more overlook like boolean data.\n",
    "- So, we are replacing Country column as vector eg : ```(0, 0, 1)``` with the help of ```ColumnTransformer``` & ```OneHotEncoder``` similarly replacing Purchased column as ```0 & 1``` with the help of ```LabelEncoder```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform feature variable (Country)\n",
    "- Since our Country is categorical we are converting that into Vector ``` Example : (1.0 0.0 0.0) for France``` \n",
    "- We are using sklearn's ```ColumnTransformer```to transform the column from the feature and ```OneHotEncoder``` as the preprocessor to convert string data into vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])],remainder='passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform depended variable (Purchased)\n",
    "- As we know the Purchased column is more like a boolean we can label them as 0 & 1.\n",
    "- To do that Label encoding we are using sklearn's ```LabelEncoder``` which encode the dataset and transform the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test set\n",
    "- We have only one dataset. So, we can split that dataset into training & testing datasets.\n",
    "- Training set :\n",
    "    - More than 80% of the real dataset is considered as the training set.  \n",
    "    - This set of data is taken to train our machine learning model.\n",
    "- Testing set :\n",
    "    - Balance 20% of the real dataset is our testing set.  \n",
    "    - We are going to use this set for our testing purpose.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) # random_state=1 to don't shuffle the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train\n",
      "[[1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]]\n",
      "Y Train\n",
      "[1 1 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"X Train\", x_train, \"Y Train\", y_train, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Test\n",
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "Y Test\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"X Test\", x_test, \"Y Test\", y_test, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "- What?\n",
    "    - Feature scaling is the process of standardizing the column which has some higher value that will not be accepted by some of the models.\n",
    "- Why?\n",
    "    - It is not necessary to do feature scaling for all algorithms, but some of the algorithms expect scaled features for better performance.\n",
    "- When?\n",
    "    - Most people get confused on when to perform feature scaling, like before splitting of the dataset or after that. The answer is after the split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we only take the large value column which is not in the range of (-3, 3)\n",
    "x_train[:, 3:4+1] = sc.fit_transform(x_train[:, 3:4+1])\n",
    "x_test[:, 3:4+1] = sc.transform(x_test[:, 3:4+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 1.1368221506758058 1.162126451226079]\n",
      " [1.0 0.0 0.0 -0.4972307410396899 0.11183643672886977]\n",
      " [0.0 0.0 1.0 -0.23314138480284197 -1.2010260813926417]\n",
      " [0.0 0.0 1.0 -0.348680478156463 -0.4133085705197348]\n",
      " [0.0 1.0 0.0 -0.05157995239000922 -0.17018588197871387]\n",
      " [0.0 0.0 1.0 -1.982733369871959 -1.5511227528917113]\n",
      " [1.0 0.0 0.0 0.5426210991428984 0.5494572761027069]\n",
      " [0.0 1.0 0.0 1.4339226764422597 1.5122231227251486]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.5370825812222781 -1.0259777456431067]\n",
      " [1.0 0.0 0.0 -0.7943312668061437 -0.6758810741440371]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
