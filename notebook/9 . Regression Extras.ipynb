{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6b2124",
   "metadata": {},
   "source": [
    "# Regression Extras\n",
    "- In this notebook, we are going to look at some of the extra topics which may help you to make the model efficient.\n",
    "    1. What is statistical significance & p-value?\n",
    "    2. Building best model\n",
    "    3. Adjusted R squared factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65d845",
   "metadata": {},
   "source": [
    "## What is statistical significance & p-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1df92",
   "metadata": {},
   "source": [
    "- Let assume we are in the world, where the coin has a head and tail as faces so, we have 50-50 changes to get each face.  \n",
    "- We are tossing the same coin 6 times, let see the result in the given table.\n",
    "\n",
    "| Coin | Result | Probability of getting current result in H0 |\n",
    "|------|--------|-------------------------------------------|\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 1st time you got tails üòÄ | 0.5 (50%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 2nd time you got tails üôÇ | 0.25 (25%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 3rd time you got tails, again üòÆ | 0.125 (12%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 4th time.... tails üòë | 0.0625 (6%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 5th time, tails ü§î | 0.03125 (3%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 6th time, tails again üßê | 0.015625 (1%) |\n",
    "\n",
    "- Once you see the above table, we may think **\"Is that coin is fake or does it have both side tails? üßê\".**\n",
    "- Just check at what time you feel more suspicious about the result? I feel suspicious at 4th time tossing.\n",
    "- And you can see that, probability of getting tails is decreasing every time by half of previous.\n",
    "- We humans can get the suspicious feeling but, how about the machines or algorithms?\n",
    "- To answer this question we are creating a new variable or value called **p-value**. For our case, we can fix the **p-value as 0.05 (5%)**.\n",
    "- if our algorithm gets a p-value less than 0.5 then we can confirm given data is not useful or not fitting data to the algorithm.\n",
    "- then we can also assume that the other 95% of data is correct and valid for the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea429a7",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.analytics-toolkit.com/wp-content/uploads/2017/09/2017-09-11-Statistical-Significance-P-Value-1.png\" alt=\"p-value image\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4006b0",
   "metadata": {},
   "source": [
    "## Building best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bde3fd",
   "metadata": {},
   "source": [
    "- If you have one feature x(1) to predict the dependent variable y then, we can use Simple Linear Regression. \n",
    "- If you have many feature x(n) to predict dependent variable y then, we can use Multiple Linear Regression and so many regressions we learned. \n",
    "- But, how we can find unwanted features which completely useless for the prediction of y?\n",
    "\n",
    "```\n",
    "example:\n",
    "Let assume we are going to predict \"Profit\" (y)\n",
    "Which is dependent on \n",
    "1. \"R&D Spend\" of the company.\n",
    "2. \"Administration Spend\"  of the company.\n",
    "3. \"Marketing Spend\" of the company.\n",
    "4. \"State\" where the company is located.\n",
    "```\n",
    "\n",
    "- Can you guess! What are the best set of feature variables that is most dependent for predicting \"Profit\"?\n",
    "- Let's find it out üòé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2f3ce",
   "metadata": {},
   "source": [
    "### 5 methods of model building\n",
    "1. All-in\n",
    "2. Backward elimination (Stepwise Regression)\n",
    "3. Forward selection (Stepwise Regression)\n",
    "4. Bidirectional elimination (Stepwise Regression)\n",
    "5. All Possible Model (Score Comparision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9ac37",
   "metadata": {},
   "source": [
    "#### 1. All-in\n",
    "- If you have prior knowledge about the dataset and you are sure that all y is dependent on all the feature variables.\n",
    "- If someone gives you a completely perfect dataset, then in that case you have to use all feature variables.\n",
    "- We do **All-in** before going to *Backward elimination*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69c7e2",
   "metadata": {},
   "source": [
    "#### 2. Backward elimination\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay** in the model. ```example: SL_STAY = 0.05 (5%)```\n",
    "- **STEP 2**: Perform *All-in* with all possible feature varibales.\n",
    "- **STEP 3**: Find p-value for each feature. If ```p > SL_STAY``` goto **STEP 4** else **END**.\n",
    "- **STEP 4**: Remove the feature\n",
    "- **STEP 5**: Refit the model with new set of feature and continue to **STEP 3**.\n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02cd50",
   "metadata": {},
   "source": [
    "#### 3. Forward selection\n",
    "- **STEP 1**: You have to select *statistical significance* level to **enter** in the model. ```example: SL_ENTER = 0.05 (5%)```\n",
    "- **STEP 2**: Find the best simple linear regression model but apply every single feature x(n) with the y.\n",
    "- **STEP 3**: Keep that selected feature in the model and try adding all other features one by one.\n",
    "- **STEP 4**: Find p-value for each feature. If ```p < SL_ENTER``` goto **STEP 3** else **END**.\n",
    "- **END**: ü•≥ Keep your previous, that's the model your look for ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823992a",
   "metadata": {},
   "source": [
    "#### 4. Bidirectional elimination\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay & enter** in the model. ```example:  SL_ENTER = 0.05 (5%) & SL_STAY = 0.05 (5%)```\n",
    "- **STEP 2**: Perform **Forward selection** to select feature variable set with (SL_ENTER = 0.05).\n",
    "- **STEP 3**: Perform all steps in **Backward elimination** on the selected set with (SL_STAY = 0.05) and continue to **STEP 2**.\n",
    "- **STEP 4**: Iteration of **STEP 3 & 4** will be continue until no variable added or exit from the model then **END**. \n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae0c29",
   "metadata": {},
   "source": [
    "#### 5. All Possible Model\n",
    "- **STEP 1**: Select one goodness criteria ```example: R^2```\n",
    "- **STEP 2**: Construct all possible models from the N feature ```ie, N feature can have (2^N)-1 total combinations```\n",
    "- **STEP 3**: Find the best model out of it by applying criteria\n",
    "- **END**: ü•≥ Your model is ready ü•≥\n",
    "\n",
    "> Note : If you have 10 feature then you need to find 1023 models to take best out of it üò´."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d5ef2",
   "metadata": {},
   "source": [
    "## Adjust R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307659a2",
   "metadata": {},
   "source": [
    "- We all learned about the [R squared](https://github.com/sanjaysanju618/Machine-learning/blob/main/notebook/8%20.%20Regression%20Model%20Selection.ipynb), which is the great factor that helps us to evacuate the model performance.\n",
    "\n",
    "<img src=\"../images/r_squared_eqn.png\" alt=\"r_squared_eqn.png\" width=\"500\">\n",
    "\n",
    "- But, there is one problem with it! Guess what? Answer this question \"What will the result of R squared value if you add a new feature to the model?\"\n",
    "- The answer is your R squared value also increases! Why? You may think the new variable is not much import for prediction, but that feature is having a very small impact on prediction. Let say about 0.0001 % of dependence.\n",
    "- Then how we find the performace of new model ü§î?\n",
    "- Here come's our hero **Adjust R squared** üòé.\n",
    "\n",
    "<img src=\"../images/adj_r_squared_eqn.png\" alt=\"adj_r_squared_eqn.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
