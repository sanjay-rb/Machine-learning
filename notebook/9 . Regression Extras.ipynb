{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression Extras\r\n",
    "- In this notebook, we are going to look at some of the extra topics which may help you to make the model efficient.\r\n",
    "    1. What is statistical significance & p-value?\r\n",
    "    2. Building best model\r\n",
    "    3. Adjusted R squared factor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is statistical significance & p-value?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let assume we are in the world, where the coin has a head and tail as faces so, we have 50-50 changes to get each face.  \r\n",
    "- We are tossing the same coin 6 times, let see the result in the given table.\r\n",
    "\r\n",
    "| Coin | Result | Probability of getting current result in H0 |\r\n",
    "|------|--------|-------------------------------------------|\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 1st time you got tails üòÄ | 0.5 (50%) |\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 2nd time you got tails üôÇ | 0.25 (25%) |\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 3rd time you got tails, again üòÆ | 0.125 (12%) |\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 4th time.... tails üòë | 0.0625 (6%) |\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 5th time, tails ü§î | 0.03125 (3%) |\r\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 6th time, tails again üßê | 0.015625 (1%) |\r\n",
    "\r\n",
    "- Once you see the above table, we may think **\"Is that coin is fake or does it have both side tails? üßê\".**\r\n",
    "- Just check at what time you feel more suspicious about the result? I feel suspicious at 4th time tossing.\r\n",
    "- And you can see that, probability of getting tails is decreasing every time by half of previous.\r\n",
    "- We humans can get the suspicious feeling but, how about the machines or algorithms?\r\n",
    "- To answer this question we are creating a new variable or value called **p-value**. For our case, we can fix the **p-value as 0.05 (5%)**.\r\n",
    "- if our algorithm gets a p-value less than 0.5 then we can confirm given data is not useful or not fitting data to the algorithm.\r\n",
    "- then we can also assume that the other 95% of data is correct and valid for the algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://blog.analytics-toolkit.com/wp-content/uploads/2017/09/2017-09-11-Statistical-Significance-P-Value-1.png\" alt=\"p-value image\" width=\"500\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building best model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If you have one feature x(1) to predict the dependent variable y then, we can use Simple Linear Regression. \r\n",
    "- If you have many feature x(n) to predict dependent variable y then, we can use Multiple Linear Regression and so many regressions we learned. \r\n",
    "- But, how we can find unwanted features which completely useless for the prediction of y?\r\n",
    "```\r\n",
    "example:\r\n",
    "Let assume we are going to predict \"Profit\" (y)\r\n",
    "Which is dependent on \r\n",
    "1. \"R&D Spend\" of the company.\r\n",
    "2. \"Administration Spend\"  of the company.\r\n",
    "3. \"Marketing Spend\" of the company.\r\n",
    "4. \"State\" where the company is located.\r\n",
    "```\r\n",
    "- Can you guess! What are the best set of feature variables that is most dependent for predicting \"Profit\"?\r\n",
    "- Let's find it out üòé."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 methods of model building\r\n",
    "1. All-in\r\n",
    "2. Backward elimination (Stepwise Regression)\r\n",
    "3. Forward selection (Stepwise Regression)\r\n",
    "4. Bidirectional elimination (Stepwise Regression)\r\n",
    "5. All Possible Model (Score Comparision)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. All-in\r\n",
    "- If you have prior knowledge about the dataset and you are sure that all y is dependent on all the feature variables.\r\n",
    "- If someone gives you a completely perfect dataset, then in that case you have to use all feature variables.\r\n",
    "- We do **All-in** before going to *Backward elimination*. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Backward elimination\r\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay** in the model. ```example: SL_STAY = 0.05 (5%)```\r\n",
    "- **STEP 2**: Perform *All-in* with all possible feature varibales.\r\n",
    "- **STEP 3**: Find p-value for each feature. If ```p > SL_STAY``` goto **STEP 4** else **END**.\r\n",
    "- **STEP 4**: Remove the feature\r\n",
    "- **STEP 5**: Refit the model with new set of feature and continue to **STEP 3**.\r\n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Forward selection\r\n",
    "- **STEP 1**: You have to select *statistical significance* level to **enter** in the model. ```example: SL_ENTER = 0.05 (5%)```\r\n",
    "- **STEP 2**: Find the best simple linear regression model but apply every single feature x(n) with the y.\r\n",
    "- **STEP 3**: Keep that selected feature in the model and try adding all other features one by one.\r\n",
    "- **STEP 4**: Find p-value for each feature. If ```p < SL_ENTER``` goto **STEP 3** else **END**.\r\n",
    "- **END**: ü•≥ Keep your previous, that's the model your look for ü•≥"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Bidirectional elimination\r\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay & enter** in the model. ```example:  SL_ENTER = 0.05 (5%) & SL_STAY = 0.05 (5%)```\r\n",
    "- **STEP 2**: Perform **Forward selection** to select feature variable set with (SL_ENTER = 0.05).\r\n",
    "- **STEP 3**: Perform all steps in **Backward elimination** on the selected set with (SL_STAY = 0.05) and continue to **STEP 2**.\r\n",
    "- **STEP 4**: Iteration of **STEP 3 & 4** will be continue until no variable added or exit from the model then **END**. \r\n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. All Possible Model\r\n",
    "- **STEP 1**: Select one goodness criteria ```example: R^2```\r\n",
    "- **STEP 2**: Construct all possible models from the N feature ```ie, N feature can have (2^N)-1 total combinations```\r\n",
    "- **STEP 3**: Find the best model out of it by applying criteria\r\n",
    "- **END**: ü•≥ Your model is ready ü•≥\r\n",
    "\r\n",
    "> Note : If you have 10 feature then you need to find 1023 models to take best out of it üò´."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adjust R squared"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- We all learned about the [R squared](https://render.githubusercontent.com/view/ipynb?color_mode=light&commit=926f6a3db7d36af0e4b5a5e1760a7ec2366cb4a1&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73616e6a617973616e6a753631382f4d616368696e652d6c6561726e696e672f393236663661336462376433366166306534623561356531373630613765633233363663623461312f6e6f7465626f6f6b2f382532302e25323052656772657373696f6e2532304d6f64656c25323053656c656374696f6e2e6970796e62&nwo=sanjaysanju618%2FMachine-learning&path=notebook%2F8+.+Regression+Model+Selection.ipynb&repository_id=402282035&repository_type=Repository#R-square-(R%5E2)), which is the great factor that helps us to evacuate the model performance.\r\n",
    "\r\n",
    "<img src=\"../images/r_squared_eqn.png\" alt=\"r_squared_eqn.png\" width=\"500\">\r\n",
    "\r\n",
    "- But, there is one problem with it! Guess what? Answer this question \"What will the result of R squared value if you add a new feature to the model?\"\r\n",
    "- The answer is your R squared value also increases! Why? You may think the new variable is not much import for prediction, but that feature is having a very small impact on prediction. Let say about 0.0001 % of dependence.\r\n",
    "- Then how we find the performace of new model ü§î?\r\n",
    "- Here come's our hero **Adjust R squared** üòé.\r\n",
    "\r\n",
    "<img src=\"../images/adj_r_squared_eqn.png\" alt=\"adj_r_squared_eqn.png\" width=\"500\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}