{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Extras\n",
    "- In this notebook, we are going to look at some of the extra topics which may help you to make the model efficient.\n",
    "    1. What is statistical significance & p-value?\n",
    "    2. Building best model\n",
    "    3. Adjusted R squared factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is statistical significance & p-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let assume we are in the world, where the coin has a head and tail as faces so, we have 50-50 changes to get each face.  \n",
    "- We are tossing the same coin 6 times, let see the result in the given table.\n",
    "\n",
    "| Coin | Result | Probability of getting current result in H0 |\n",
    "|------|--------|-------------------------------------------|\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 1st time you got tails üòÄ | 0.5 (50%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 2nd time you got tails üôÇ | 0.25 (25%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 3rd time you got tails, again üòÆ | 0.125 (12%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 4th time.... tails üòë | 0.0625 (6%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 5th time, tails ü§î | 0.03125 (3%) |\n",
    "| <img src=\"../images/coin.png\" alt=\"coin.png\" width=\"30\"> | 6th time, tails again üßê | 0.015625 (1%) |\n",
    "\n",
    "- Once you see the above table, we may think **\"Is that coin is fake or does it have both side tails? üßê\".**\n",
    "- Just check at what time you feel more suspicious about the result? I feel suspicious at 4th time tossing.\n",
    "- And you can see that, probability of getting tails is decreasing every time by half of previous.\n",
    "- We humans can get the suspicious feeling but, how about the machines or algorithms?\n",
    "- To answer this question we are creating a new variable or value called **p-value**. For our case, we can fix the **p-value as 0.05 (5%)**.\n",
    "- if our algorithm gets a p-value less than 0.5 then we can confirm given data is not useful or not fitting data to the algorithm.\n",
    "- then we can also assume that the other 95% of data is correct and valid for the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.analytics-toolkit.com/wp-content/uploads/2017/09/2017-09-11-Statistical-Significance-P-Value-1.png\" alt=\"p-value image\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you have one feature x(1) to predict the dependent variable y then, we can use Simple Linear Regression. \n",
    "- If you have many feature x(n) to predict dependent variable y then, we can use Multiple Linear Regression and so many regressions we learned. \n",
    "- But, how we can find unwanted features which completely useless for the prediction of y?\n",
    "\n",
    "```\n",
    "example:\n",
    "Let assume we are going to predict \"Profit\" (y)\n",
    "Which is dependent on \n",
    "1. \"R&D Spend\" of the company.\n",
    "2. \"Administration Spend\"  of the company.\n",
    "3. \"Marketing Spend\" of the company.\n",
    "4. \"State\" where the company is located.\n",
    "```\n",
    "\n",
    "- Can you guess! What are the best set of feature variables that is most dependent for predicting \"Profit\"?\n",
    "- Let's find it out üòé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 methods of model building\n",
    "1. All-in\n",
    "2. Backward elimination (Stepwise Regression)\n",
    "3. Forward selection (Stepwise Regression)\n",
    "4. Bidirectional elimination (Stepwise Regression)\n",
    "5. All Possible Model (Score Comparision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. All-in\n",
    "- If you have prior knowledge about the dataset and you are sure that all y is dependent on all the feature variables.\n",
    "- If someone gives you a completely perfect dataset, then in that case you have to use all feature variables.\n",
    "- We do **All-in** before going to *Backward elimination*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Backward elimination\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay** in the model. ```example: SL_STAY = 0.05 (5%)```\n",
    "- **STEP 2**: Perform *All-in* with all possible feature varibales.\n",
    "- **STEP 3**: Find p-value for each feature. If ```p > SL_STAY``` goto **STEP 4** else **END**.\n",
    "- **STEP 4**: Remove the feature\n",
    "- **STEP 5**: Refit the model with new set of feature and continue to **STEP 3**.\n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Forward selection\n",
    "- **STEP 1**: You have to select *statistical significance* level to **enter** in the model. ```example: SL_ENTER = 0.05 (5%)```\n",
    "- **STEP 2**: Find the best simple linear regression model but apply every single feature x(n) with the y.\n",
    "- **STEP 3**: Keep that selected feature in the model and try adding all other features one by one.\n",
    "- **STEP 4**: Find p-value for each feature. If ```p < SL_ENTER``` goto **STEP 3** else **END**.\n",
    "- **END**: ü•≥ Keep your previous, that's the model your look for ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bidirectional elimination\n",
    "- **STEP 1**: You have to select *statistical significance* level to **stay & enter** in the model. ```example:  SL_ENTER = 0.05 (5%) & SL_STAY = 0.05 (5%)```\n",
    "- **STEP 2**: Perform **Forward selection** to select feature variable set with (SL_ENTER = 0.05).\n",
    "- **STEP 3**: Perform all steps in **Backward elimination** on the selected set with (SL_STAY = 0.05) and continue to **STEP 2**.\n",
    "- **STEP 4**: Iteration of **STEP 3 & 4** will be continue until no variable added or exit from the model then **END**. \n",
    "- **END**: ü•≥ Your model is ready ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. All Possible Model\n",
    "- **STEP 1**: Select one goodness criteria ```example: R^2```\n",
    "- **STEP 2**: Construct all possible models from the N feature ```ie, N feature can have (2^N)-1 total combinations```\n",
    "- **STEP 3**: Find the best model out of it by applying criteria\n",
    "- **END**: ü•≥ Your model is ready ü•≥\n",
    "\n",
    "> Note : If you have 10 feature then you need to find 1023 models to take best out of it üò´."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We all learned about the [R squared](https://github.com/sanjaysanju618/Machine-learning/blob/main/notebook/8%20.%20Regression%20Model%20Selection.ipynb), which is the great factor that helps us to evacuate the model performance.\n",
    "\n",
    "<img src=\"../images/r_squared_eqn.png\" alt=\"r_squared_eqn.png\" width=\"500\">\n",
    "\n",
    "- But, there is one problem with it! Guess what? Answer this question \"What will the result of R squared value if you add a new feature to the model?\"\n",
    "- The answer is your R squared value also increases! Why? You may think the new variable is not much import for prediction, but that feature is having a very small impact on prediction. Let say about 0.0001 % of dependence.\n",
    "- Then how do we find the performance of model ü§î?\n",
    "- Here come's our hero **Adjust R squared** üòé.\n",
    "\n",
    "<img src=\"../images/adj_r_squared_eqn.png\" alt=\"adj_r_squared_eqn.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Adjusted R squared adjusting for the model?\n",
    "- Well, We have p and n for that.\n",
    "- p is the number of regressors (feature), n is the total size of the sample (dataset).\n",
    "- By adding new variable ```p``` is increases, ```(n-1)/(n-1-p)``` increases.\n",
    "- By adding new variable ```R^2``` is increases, ```1-R^2``` decreases.\n",
    "- there will be a battle between the two equations. It will finally compensate with a subtraction of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lots of theories, Let Code ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "‚úîÔ∏è Import the necessary libraries.\n",
    "\n",
    "‚úîÔ∏è Load dataset (Combined_Cycle_Power_Plant.csv).\n",
    "\n",
    "‚ùå Our dataset doesn't have any missing data.\n",
    "\n",
    "‚ùå We have categorical string data.\n",
    "\n",
    "‚úîÔ∏è We have 9569 data. So, we can split this dataset into testing and training datasets to evaluate the result.\n",
    "\n",
    "‚ö†Ô∏è Please apply feature scaling only if required by the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries....\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2  136897.8  471784.1 ]\n",
      " [162597.7  151377.59 443898.53]\n",
      " [153441.51 101145.55 407934.54]\n",
      " [144372.41 118671.85 383199.62]\n",
      " [142107.34  91391.77 366168.42]\n",
      " [131876.9   99814.71 362861.36]\n",
      " [134615.46 147198.87 127716.82]\n",
      " [130298.13 145530.06 323876.68]\n",
      " [120542.52 148718.95 311613.29]\n",
      " [123334.88 108679.17 304981.62]\n",
      " [101913.08 110594.11 229160.95]\n",
      " [100671.96  91790.61 249744.55]\n",
      " [ 93863.75 127320.38 249839.44]\n",
      " [ 91992.39 135495.07 252664.93]\n",
      " [119943.24 156547.42 256512.92]\n",
      " [114523.61 122616.84 261776.23]\n",
      " [ 78013.11 121597.55 264346.06]\n",
      " [ 94657.16 145077.58 282574.31]\n",
      " [ 91749.16 114175.79 294919.57]\n",
      " [ 86419.7  153514.11      0.  ]\n",
      " [ 76253.86 113867.3  298664.47]\n",
      " [ 78389.47 153773.43 299737.29]\n",
      " [ 73994.56 122782.75 303319.26]\n",
      " [ 67532.53 105751.03 304768.73]\n",
      " [ 77044.01  99281.34 140574.81]\n",
      " [ 64664.71 139553.16 137962.62]\n",
      " [ 75328.87 144135.98 134050.07]\n",
      " [ 72107.6  127864.55 353183.81]\n",
      " [ 66051.52 182645.56 118148.2 ]\n",
      " [ 65605.48 153032.06 107138.38]\n",
      " [ 61994.48 115641.28  91131.24]\n",
      " [ 61136.38 152701.92  88218.23]\n",
      " [ 63408.86 129219.61  46085.25]\n",
      " [ 55493.95 103057.49 214634.81]\n",
      " [ 46426.07 157693.92 210797.67]\n",
      " [ 46014.02  85047.44 205517.64]\n",
      " [ 28663.76 127056.21 201126.82]\n",
      " [ 44069.95  51283.14 197029.42]\n",
      " [ 20229.59  65947.93 185265.1 ]\n",
      " [ 38558.51  82982.09 174999.3 ]\n",
      " [ 28754.33 118546.05 172795.67]\n",
      " [ 27892.92  84710.77 164470.71]\n",
      " [ 23640.93  96189.63 148001.11]\n",
      " [ 15505.73 127382.3   35534.17]\n",
      " [ 22177.74 154806.14  28334.72]\n",
      " [  1000.23 124153.04   1903.93]\n",
      " [  1315.46 115816.21 297114.46]\n",
      " [     0.   135426.92      0.  ]\n",
      " [   542.05  51743.15      0.  ]\n",
      " [     0.   116983.8   45173.06]]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset....\n",
    "dataset = pd.read_csv(r\"../dataset/50_Startups_Updated.csv\")\n",
    "X = dataset.iloc[:, :-1].values # [row, column]\n",
    "Y = dataset.iloc[:, -1].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using statsmodels module for get stats of the model....\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- statsmodels module only take raw data and perform operations\n",
    "- let's say we have \n",
    "- y = b0 * x0 + b1 * x1 + b2 * x2 + b3 * x3\n",
    "- where x0 is always 1\n",
    "- our normal regression model will understand it automatically and consider x0 = 1.\n",
    "- but statsmodels module don't take x0 as 1 we need to add extra 1 column at begin of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 1.65e+05 1.37e+05 4.72e+05]\n",
      " [1.00e+00 1.63e+05 1.51e+05 4.44e+05]\n",
      " [1.00e+00 1.53e+05 1.01e+05 4.08e+05]\n",
      " [1.00e+00 1.44e+05 1.19e+05 3.83e+05]\n",
      " [1.00e+00 1.42e+05 9.14e+04 3.66e+05]\n",
      " [1.00e+00 1.32e+05 9.98e+04 3.63e+05]\n",
      " [1.00e+00 1.35e+05 1.47e+05 1.28e+05]\n",
      " [1.00e+00 1.30e+05 1.46e+05 3.24e+05]\n",
      " [1.00e+00 1.21e+05 1.49e+05 3.12e+05]\n",
      " [1.00e+00 1.23e+05 1.09e+05 3.05e+05]\n",
      " [1.00e+00 1.02e+05 1.11e+05 2.29e+05]\n",
      " [1.00e+00 1.01e+05 9.18e+04 2.50e+05]\n",
      " [1.00e+00 9.39e+04 1.27e+05 2.50e+05]\n",
      " [1.00e+00 9.20e+04 1.35e+05 2.53e+05]\n",
      " [1.00e+00 1.20e+05 1.57e+05 2.57e+05]\n",
      " [1.00e+00 1.15e+05 1.23e+05 2.62e+05]\n",
      " [1.00e+00 7.80e+04 1.22e+05 2.64e+05]\n",
      " [1.00e+00 9.47e+04 1.45e+05 2.83e+05]\n",
      " [1.00e+00 9.17e+04 1.14e+05 2.95e+05]\n",
      " [1.00e+00 8.64e+04 1.54e+05 0.00e+00]\n",
      " [1.00e+00 7.63e+04 1.14e+05 2.99e+05]\n",
      " [1.00e+00 7.84e+04 1.54e+05 3.00e+05]\n",
      " [1.00e+00 7.40e+04 1.23e+05 3.03e+05]\n",
      " [1.00e+00 6.75e+04 1.06e+05 3.05e+05]\n",
      " [1.00e+00 7.70e+04 9.93e+04 1.41e+05]\n",
      " [1.00e+00 6.47e+04 1.40e+05 1.38e+05]\n",
      " [1.00e+00 7.53e+04 1.44e+05 1.34e+05]\n",
      " [1.00e+00 7.21e+04 1.28e+05 3.53e+05]\n",
      " [1.00e+00 6.61e+04 1.83e+05 1.18e+05]\n",
      " [1.00e+00 6.56e+04 1.53e+05 1.07e+05]\n",
      " [1.00e+00 6.20e+04 1.16e+05 9.11e+04]\n",
      " [1.00e+00 6.11e+04 1.53e+05 8.82e+04]\n",
      " [1.00e+00 6.34e+04 1.29e+05 4.61e+04]\n",
      " [1.00e+00 5.55e+04 1.03e+05 2.15e+05]\n",
      " [1.00e+00 4.64e+04 1.58e+05 2.11e+05]\n",
      " [1.00e+00 4.60e+04 8.50e+04 2.06e+05]\n",
      " [1.00e+00 2.87e+04 1.27e+05 2.01e+05]\n",
      " [1.00e+00 4.41e+04 5.13e+04 1.97e+05]\n",
      " [1.00e+00 2.02e+04 6.59e+04 1.85e+05]\n",
      " [1.00e+00 3.86e+04 8.30e+04 1.75e+05]\n",
      " [1.00e+00 2.88e+04 1.19e+05 1.73e+05]\n",
      " [1.00e+00 2.79e+04 8.47e+04 1.64e+05]\n",
      " [1.00e+00 2.36e+04 9.62e+04 1.48e+05]\n",
      " [1.00e+00 1.55e+04 1.27e+05 3.55e+04]\n",
      " [1.00e+00 2.22e+04 1.55e+05 2.83e+04]\n",
      " [1.00e+00 1.00e+03 1.24e+05 1.90e+03]\n",
      " [1.00e+00 1.32e+03 1.16e+05 2.97e+05]\n",
      " [1.00e+00 0.00e+00 1.35e+05 0.00e+00]\n",
      " [1.00e+00 5.42e+02 5.17e+04 0.00e+00]\n",
      " [1.00e+00 0.00e+00 1.17e+05 4.52e+04]]\n"
     ]
    }
   ],
   "source": [
    "# Adding x0=1 at beginning of the dataset\n",
    "X = np.append(arr = np.ones((len(X), 1)).astype(int), values = X, axis = 1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let find out the best fit model with the help of Backward elimination.\n",
    "- Why Backward elimination? It is the fastest and best algorithm among all to start. All other algorithms need better mathematical understanding or super-fast computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assign some variables for our understand....\n",
    "X0_Coefficient=0\n",
    "R_and_D_Spend=1\n",
    "Administration_Spend=2\n",
    "Marketing_Spend=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Oct 2021</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:35:28</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     296.0\n",
       "Date:                Sat, 09 Oct 2021   Prob (F-statistic):           4.53e-30\n",
       "Time:                        09:35:28   Log-Likelihood:                -525.39\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      46   BIC:                             1066.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
       "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
       "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
       "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
       "==============================================================================\n",
       "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
       "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
       "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing X_opt with [X0_Coefficient, R_and_D_Spend, Administration_Spend, Marketing_Spend] columns...\n",
    "X_opt = X[:, [X0_Coefficient, R_and_D_Spend, Administration_Spend, Marketing_Spend]]\n",
    "X_opt = X_opt.astype(np.float64)\n",
    "\n",
    "# Fit regression model using \"Ordinary Least Squares\" algorithm...\n",
    "regressor_OLS = sm.OLS(Y, X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result review 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/p_value_and_adj_r_squared_highlights.png\" alt=\"p_value_and_adj_r_squared_highlights.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P-value for x2 (Administration_Spend) is 0.602 > (SL) 0.05, completely failed with higher difference. ‚ùå\n",
    "- Adj. R-squared:\t0.948, More close to 1 great model. ‚úîÔ∏è\n",
    "> So, we are removing **Administration_Spend** from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Oct 2021</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:35:28</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     450.8\n",
       "Date:                Sat, 09 Oct 2021   Prob (F-statistic):           2.16e-31\n",
       "Time:                        09:35:28   Log-Likelihood:                -525.54\n",
       "No. Observations:                  50   AIC:                             1057.\n",
       "Df Residuals:                      47   BIC:                             1063.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
       "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
       "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
       "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
       "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing X_opt with [X0_Coefficient, R_and_D_Spend, Marketing_Spend] columns...\n",
    "X_opt = X[:, [X0_Coefficient, R_and_D_Spend, Marketing_Spend]]\n",
    "X_opt = X_opt.astype(np.float64)\n",
    "\n",
    "# Fit regression model using \"Ordinary Least Squares\" algorithm...\n",
    "regressor_OLS = sm.OLS(Y, X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result review 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P-value for x2 (Marketing_Spend) is 0.060 > (SL) 0.05, too close to SL. ‚ö†Ô∏è\n",
    "- Adj. R-squared:\t0.948, same closer Adj. R-squared. ‚úîÔ∏è\n",
    "> So, we are removing **Marketing_Spend** from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Oct 2021</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:35:28</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     849.8\n",
       "Date:                Sat, 09 Oct 2021   Prob (F-statistic):           3.50e-32\n",
       "Time:                        09:35:28   Log-Likelihood:                -527.44\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      48   BIC:                             1063.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
       "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
       "==============================================================================\n",
       "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
       "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
       "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing X_opt with [X0_Coefficient, R_and_D_Spend] columns...\n",
    "X_opt = X[:, [X0_Coefficient, R_and_D_Spend]]\n",
    "X_opt = X_opt.astype(np.float64)\n",
    "\n",
    "# Fit regression model using \"Ordinary Least Squares\" algorithm...\n",
    "regressor_OLS = sm.OLS(Y, X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result review 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P-value for x1 (R_and_D_Spend) is 0.000 < (SL) 0.05, p-value under the SL level. ‚úîÔ∏è\n",
    "- Adj. R-squared:   0.945, Goodness of fit reduced. ‚ùå\n",
    "> We understand that while removing **Marketing_Spend** Adj. R-squared is reducing! So, we can take the previous X_opt value as the final feature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final selected model \n",
    "- Hence, the last **Result review 3** of [X0_Coefficient, R_and_D_Spend] reduces the Adj R-squared.\n",
    "- Out of 3 results, we understand that **Result review 2** have great *Goodness of fit* and *very close p-value*.\n",
    "- So, we can confirm that Profit is more dependent on features [X0_Coefficient, R_and_D_Spend, Marketing_Spend]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected....\n",
    "X_opt = X[:, [X0_Coefficient, R_and_D_Spend, Marketing_Spend]]\n",
    "X_opt = X_opt.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let, create one multiple linear regression model to check the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare testing and training dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_opt, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Multiple Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----y_pred vs y_test-----\n",
      " [[102284.65 103282.38]\n",
      " [133873.92 144259.4 ]\n",
      " [134182.15 146121.95]\n",
      " [ 73701.11  77798.83]\n",
      " [180642.25 191050.39]\n",
      " [114717.25 105008.31]\n",
      " [ 68335.08  81229.06]\n",
      " [ 97433.46  97483.56]\n",
      " [114580.92 110352.25]\n",
      " [170343.32 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "# Check training dataset with testing dataset\n",
    "y_pred = regressor.predict(x_test)\n",
    "y_pred_vertical = y_pred.reshape(len(y_pred), 1)\n",
    "y_test_vertical = y_test.reshape(len(y_test), 1)\n",
    "print(\"-----y_pred vs y_test-----\\n\", np.concatenate((y_pred_vertical, y_test_vertical), 1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
