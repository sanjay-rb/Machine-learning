{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00ca570",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae7364",
   "metadata": {},
   "source": [
    "- Hierarchical Clustering have two types of algorithms.\n",
    "    1. Agglomerative Clustering\n",
    "    2. Divisive clustering\n",
    "- Agglomerative Clustering is bottom-up approach\n",
    "- Divisive Clustering is top-down approach \n",
    "- Let's see about **Agglomerative Clustering** with the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e3b59",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b1e0f",
   "metadata": {},
   "source": [
    "- **Step 1** : Every point in dataset will be a unique cluster.\n",
    "- **Step 2** : Find two closest clusters and join them together as one cluster.\n",
    "- **Step 3** : Repeat **Step 2** until there is one cluster. If there is only one cluster for dataset given then **END**.\n",
    "- **END** : Your model is ready.\n",
    "\n",
    "> NOTE: Here we need to find \"closest clusters\" not distance between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cedcec",
   "metadata": {},
   "source": [
    "## Manual Example üòï"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c27630",
   "metadata": {},
   "source": [
    "|||\n",
    "|:---:|:---:|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_1.png\" alt=\"hierarchical_clustering_ppt_1.png\" width=\"400\">|<img src=\"../static/hierarchical_clustering_ppt_2.png\" alt=\"hierarchical_clustering_ppt_2.png\" width=\"400\">|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_3.png\" alt=\"hierarchical_clustering_ppt_3.png\" width=\"400\">|<img src=\"../static/hierarchical_clustering_ppt_4.png\" alt=\"hierarchical_clustering_ppt_4.png\" width=\"400\">|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_5.png\" alt=\"hierarchical_clustering_ppt_5.png\" width=\"400\">|<img src=\"../static/hierarchical_clustering_ppt_6.png\" alt=\"hierarchical_clustering_ppt_6.png\" width=\"400\">|\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcef9d",
   "metadata": {},
   "source": [
    "## Dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you see about manual steps. Probably you may have these questions! \n",
    "    > **Why are we creating single cluster from data points?**\n",
    "- To answer this we need to draw a graph called **_Dendrograms_**.\n",
    "\n",
    "|||\n",
    "|:---:|:---:|\n",
    "||We can plot all data points in X axis & euclidean distance in Y axis|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_1.png\" alt=\"hierarchical_clustering_ppt_1.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_1.png\" alt=\"hc_dendrograms_ppt_1.png\" width=\"400\">|\n",
    "||When we connect these two points we can also connect those point in graph with height as the distance between them|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_2.png\" alt=\"hierarchical_clustering_ppt_2.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_2.png\" alt=\"hc_dendrograms_ppt_2.png\" width=\"400\">|\n",
    "||When we connect these two points observe that height of them is higher than the previous one, because distance between them is higher than first cluster|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_3.png\" alt=\"hierarchical_clustering_ppt_3.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_3.png\" alt=\"hc_dendrograms_ppt_3.png\" width=\"400\">|\n",
    "||This is interesting, when we add new point to the cluster it's look something like this üòÉ |\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_4.png\" alt=\"hierarchical_clustering_ppt_4.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_4.png\" alt=\"hc_dendrograms_ppt_4.png\" width=\"400\">|\n",
    "||This is like a building block right! üè¶|\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_5.png\" alt=\"hierarchical_clustering_ppt_5.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_5.png\" alt=\"hc_dendrograms_ppt_5.png\" width=\"400\">|\n",
    "||And we have our final **Dendrograms** |\n",
    "|<img src=\"../static/hierarchical_clustering_ppt_6.png\" alt=\"hierarchical_clustering_ppt_6.png\" width=\"400\">|<img src=\"../static/hc_dendrograms_ppt_6.png\" alt=\"hc_dendrograms_ppt_6.png\" width=\"400\">|\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653a0d2",
   "metadata": {},
   "source": [
    "## Using Dendrograms In Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99473a",
   "metadata": {},
   "source": [
    "- Are you getting confuse, why you learn, Dendrograms?\n",
    "- No worries! Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08243ee",
   "metadata": {},
   "source": [
    "As we can see our Dendrograms for Hierarchical Cluster is look like this.\n",
    "\n",
    "<img src=\"../static/hc_dendrograms_ppt_6.png\" alt=\"hc_dendrograms_ppt_6.png\" width=\"400\">\n",
    "\n",
    "Where we can **Dissimilarity Threshold** to take check cluster to fix with.\n",
    "\n",
    "- If we have Threshold at here then we will be having 2 Clusters\n",
    "\n",
    "    <img src=\"../static/hierarchical_clustering_ppt_5.png\" alt=\"hierarchical_clustering_ppt_5.png\" width=\"400\">\n",
    "    <img src=\"../static/hc_dissimilarity_threshold_1.png\" alt=\"hc_dissimilarity_threshold_1.png\" width=\"400\">\n",
    "\n",
    "- If we have Threshold at here then we have 4 cluster separation\n",
    "\n",
    "    <img src=\"../static/hierarchical_clustering_ppt_3.png\" alt=\"hierarchical_clustering_ppt_3.png\" width=\"400\">\n",
    "    <img src=\"../static/hc_dissimilarity_threshold_2.png\" alt=\"hc_dissimilarity_threshold_2.png\" width=\"400\">\n",
    "\n",
    "- Again, for this we have 6 clusters\n",
    "\n",
    "    <img src=\"../static/hierarchical_clustering_ppt_1.png\" alt=\"hierarchical_clustering_ppt_1.png\" width=\"400\">\n",
    "    <img src=\"../static/hc_dissimilarity_threshold_3.png\" alt=\"hc_dissimilarity_threshold_3.png\" width=\"400\">\n",
    "\n",
    "From above images we can understand that, if Dissimilarity Threshold Line intersect with Dendrogram Line then it is counted as one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f6c41",
   "metadata": {},
   "source": [
    "## How to find correct Dissimilarity Threshold ?\n",
    "\n",
    "We need to find the longest distance between the Dendrogram horizontal lines.\n",
    "- Follow through the images to find the trick to the longest distance.\n",
    "\n",
    "1. <img src=\"../static/hc_find_dissimilarity_threshold_1.png\" alt=\"hc_find_dissimilarity_threshold_1.png\" width=\"400\">\n",
    "2. <img src=\"../static/hc_find_dissimilarity_threshold_2.png\" alt=\"hc_find_dissimilarity_threshold_2.png\" width=\"400\">\n",
    "3. <img src=\"../static/hc_find_dissimilarity_threshold_3.png\" alt=\"hc_find_dissimilarity_threshold_3.png\" width=\"400\">\n",
    "4. <img src=\"../static/hc_find_dissimilarity_threshold_4.png\" alt=\"hc_find_dissimilarity_threshold_4.png\" width=\"400\">\n",
    "5. <img src=\"../static/hc_find_dissimilarity_threshold_5.png\" alt=\"hc_find_dissimilarity_threshold_5.png\" width=\"400\">\n",
    "6. <img src=\"../static/hc_find_dissimilarity_threshold_6.png\" alt=\"hc_find_dissimilarity_threshold_6.png\" width=\"400\">\n",
    "7. <img src=\"../static/hc_find_dissimilarity_threshold_7.png\" alt=\"hc_find_dissimilarity_threshold_7.png\" width=\"400\">\n",
    "\n",
    "Hence, we are selecting 2 clusters to predict as Dissimilarity Threshold Line intersect 2 lines in Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c8e54",
   "metadata": {},
   "source": [
    "## Lots of theory: Let's start the practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d22975",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "‚úîÔ∏è Import the necessary libraries.\n",
    "\n",
    "‚úîÔ∏è Load dataset (Mall_Customers_1.csv).\n",
    "\n",
    "‚ùå Our dataset doesn't have any missing data.\n",
    "\n",
    "‚ùå Our dataset doesn't have any string data.\n",
    "\n",
    "‚ùå As we don't have dependent variable & we are going to find new dependent variable from the dataset by clustering. We are not performing test & train data splitting.\n",
    "\n",
    "‚ùå As our dataset having range value, we are not performing feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f68142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ff296",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"../dataset/Mall_Customers_1.csv\")\n",
    "X = dataset.iloc[:, [1, 2]].values # [row, column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03659e07",
   "metadata": {},
   "source": [
    "## Draw Dendrogram Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward')) # Ward's minimum variance method is linked to Feature and given to Dendogram\n",
    "# Ward suggested a general agglomerative hierarchical clustering procedure, where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function.\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"X values\")\n",
    "plt.ylabel(\"Euclidean distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12862390",
   "metadata": {},
   "source": [
    "We need to find the longest distance between the Dendrogram horizontal lines.\n",
    "\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_1.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_1.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_2.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_2.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_3.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_3.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_4.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_4.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_5.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_5.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_6.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_6.png\" width=\"400\">\n",
    "1. <img src=\"../static/mall_customers_1_find_dissimilarity_threshold_7.png\" alt=\"mall_customers_1_find_dissimilarity_threshold_7.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we decided that no_of_cluster is 3\n",
    "no_of_cluster=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86dad0",
   "metadata": {},
   "source": [
    "## Train Hierarchical Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb29388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=no_of_cluster, linkage='ward')\n",
    "y_hc = hc.fit_predict(X)\n",
    "print(y_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aaaf51",
   "metadata": {},
   "source": [
    "## Check and visualize cluster\n",
    "- Hence, we take 2 features from Mall_Customers_1.csv. We can plot 2d graph between them.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"green\",\"blue\",\"yellow\",\"pink\",\"black\",\"orange\",\"purple\",\"beige\",\"brown\",\"gray\",\"cyan\",\"magenta\"]\n",
    "for i in range(no_of_cluster):\n",
    "    plt.scatter(X[y_hc == i, 0], X[y_hc == i, 1], s = 50, c = colors[i], label = f'Cluster {i}')\n",
    "\n",
    "plt.title(\"Cluster of customer\")\n",
    "plt.xlabel(\"Annual Income (k$)\")\n",
    "plt.ylabel(\"Spending Score (1-100)\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541a5a2",
   "metadata": {},
   "source": [
    "## Let's do the same for multiple feature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e257621",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "‚úîÔ∏è Import the necessary libraries.\n",
    "\n",
    "‚úîÔ∏è Load dataset (Mall_Customers_2.csv).\n",
    "\n",
    "‚ùå Our dataset doesn't have any missing data.\n",
    "\n",
    "‚úîÔ∏è Our dataset have string value \"Genre\" ‚Üí [\"Male\", \"Female\"], need to perform feature scaling and give label to them.\n",
    "\n",
    "‚ùå As we don't have dependent variable & we are going to find new dependent variable from the dataset by clustering. We are not performing test & train data splitting.\n",
    "\n",
    "‚ùå As our dataset having range value, we are not performing feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"../dataset/Mall_Customers_2.csv\")\n",
    "X = dataset.iloc[:, [1, 2, 3, 4]].values # [row, column]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 0] = le.fit_transform(X[:, 0])\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward')) # Ward's minimum variance method\n",
    "# Ward suggested a general agglomerative hierarchical clustering procedure, where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function.\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"X values\")\n",
    "plt.ylabel(\"Euclidean distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we decided that no_of_cluster is 3\n",
    "no_of_cluster=3\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=no_of_cluster, linkage='ward')\n",
    "y_hc = hc.fit_predict(X)\n",
    "print(y_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a27e0",
   "metadata": {},
   "source": [
    "## Check and visualize cluster\n",
    "- Hence, we have more than 2 features. We can't plot 2d graph to visualize the cluster\n",
    "- But, we can see that our dataset is generated `y_hc` with 3 clusters.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
